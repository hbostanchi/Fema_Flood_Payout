{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEMA DATA\n",
    "\n",
    "This dataset is used to determine the houses that was damaged by flood and the amount of claim that was paid by FEMA to damaged house. There are additional information for each house including lat, long, year the house was built, number of stories, city, zip code, etc. \n",
    "\n",
    "The scope if to find another dataset from NASA that includes percipitation. Then combine the 2 dataset and train the model to predict the expected \"claim\" based on \"percipitation\". Additional data will be used to better train the model including location, year that the building was built, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. READ FEMA DATA from CSV FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File FIMA_NFIP_Redacted_Claims_Data_Set/openFEMA_claims20190831.csv does not exist: 'FIMA_NFIP_Redacted_Claims_Data_Set/openFEMA_claims20190831.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-3062ae618b5d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# Import our input dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mFEMA_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'FIMA_NFIP_Redacted_Claims_Data_Set/openFEMA_claims20190831.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mFEMA_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File FIMA_NFIP_Redacted_Claims_Data_Set/openFEMA_claims20190831.csv does not exist: 'FIMA_NFIP_Redacted_Claims_Data_Set/openFEMA_claims20190831.csv'"
     ]
    }
   ],
   "source": [
    "# Import our dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import our input dataset\n",
    "FEMA_df = pd.read_csv('FIMA_NFIP_Redacted_Claims_Data_Set/openFEMA_claims20190831.csv')\n",
    "FEMA_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Remove Unnecessary Columns From FEMA Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.1. Remove columns that are obviously not adding any value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEMA_Clean_df=FEMA_df.drop(columns=[\"policycount\",\"countycode\",\"crsdiscount\",\"condominiumindicator\",\"agriculturestructureindicator\",\n",
    "                                    \"basementenclosurecrawlspacetype\",'asofdate',\"amountpaidoncontentsclaim\",\"obstructiontype\",\n",
    "                      'basefloodelevation',\"elevationcertificateindicator\",\"elevatedbuildingindicator\",\"censustract\",\n",
    "                      \"houseworship\",\"locationofcontents\",\"lowestadjacentgrade\",\"postfirmconstructionindicator\",\"yearofloss\",\n",
    "                      \"lowestfloorelevation\",\"nonprofitindicator\",\"originalnbdate\",\"amountpaidonincreasedcostofcomplianceclaim\",\n",
    "                      \"smallbusinessindicatorbuilding\",\"primaryresidence\",\"ratemethod\",\"totalbuildinginsurancecoverage\",\"totalcontentsinsurancecoverage\"])\n",
    "FEMA_Clean_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEMA_Clean_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEMA_Clean_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.2. Remove Columnms that has many empty cells and check to see remaining columns dont have many empty cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove all Data that donâ€™t have an City defined.\n",
    "FEMA_Clean1_df=FEMA_Clean_df[FEMA_Clean_df[\"reportedcity\"] != \"N/A\"]\n",
    "FEMA_Clean1_df.dropna(subset=['reportedcity'], how='all', inplace=True)\n",
    "print(FEMA_Clean1_df.shape)\n",
    "\n",
    "FEMA_Clean2_df=FEMA_Clean1_df[FEMA_Clean1_df[\"amountpaidonbuildingclaim\"] != \"N/A\"]\n",
    "FEMA_Clean2_df.dropna(subset=['amountpaidonbuildingclaim'], how='all', inplace=True)\n",
    "print(FEMA_Clean2_df.shape)\n",
    "\n",
    "FEMA_Clean3_df=FEMA_Clean2_df[FEMA_Clean2_df[\"originalconstructiondate\"] != \"N/A\"]\n",
    "FEMA_Clean3_df.dropna(subset=['originalconstructiondate'], how='all', inplace=True)\n",
    "print(FEMA_Clean3_df.shape)\n",
    "\n",
    "FEMA_Clean4_df=FEMA_Clean3_df[FEMA_Clean3_df[\"floodzone\"] != \"N/A\"]\n",
    "FEMA_Clean4_df.dropna(subset=['floodzone'], how='all', inplace=True)\n",
    "print(FEMA_Clean4_df.shape)\n",
    "\n",
    "FEMA_Clean5_df=FEMA_Clean4_df[FEMA_Clean4_df[\"occupancytype\"] != \"N/A\"]\n",
    "FEMA_Clean5_df.dropna(subset=['occupancytype'], how='all', inplace=True)\n",
    "print(FEMA_Clean5_df.shape)\n",
    "\n",
    "FEMA_Clean6_df=FEMA_Clean5_df[FEMA_Clean5_df[\"numberoffloorsintheinsuredbuilding\"] != \"N/A\"]\n",
    "FEMA_Clean6_df.dropna(subset=['numberoffloorsintheinsuredbuilding'], how='all', inplace=True)\n",
    "print(FEMA_Clean6_df.shape)\n",
    "\n",
    "FEMA_Clean7_df=FEMA_Clean6_df[FEMA_Clean6_df[\"latitude\"] != \"N/A\"]\n",
    "FEMA_Clean7_df.dropna(subset=['latitude'], how='all', inplace=True)\n",
    "print(FEMA_Clean7_df.shape)\n",
    "\n",
    "FEMA_Clean8_df=FEMA_Clean7_df[FEMA_Clean7_df[\"occupancytype\"] != \"N/A\"]\n",
    "FEMA_Clean8_df.dropna(subset=['longitude'], how='all', inplace=True)\n",
    "print(FEMA_Clean8_df.shape)\n",
    "\n",
    "FEMA_Clean9_df=FEMA_Clean8_df[FEMA_Clean8_df[\"dateofloss\"] != \"N/A\"]\n",
    "FEMA_Clean9_df.dropna(subset=['dateofloss'], how='all', inplace=True)\n",
    "print(FEMA_Clean9_df.shape)\n",
    "\n",
    "FEMA_Clean10_df=FEMA_Clean9_df[FEMA_Clean9_df[\"reportedzipcode\"] != \"N/A\"]\n",
    "FEMA_Clean10_df.dropna(subset=['reportedzipcode'], how='all', inplace=True)\n",
    "print(FEMA_Clean10_df.shape)\n",
    "\n",
    "FEMA_Clean11_df=FEMA_Clean10_df[FEMA_Clean10_df[\"reportedzipcode\"] != \"N/A\"]\n",
    "FEMA_Clean11_df.dropna(subset=['reportedzipcode'], how='all', inplace=True)\n",
    "print(FEMA_Clean11_df.shape)\n",
    "\n",
    "FEMA_Clean12_df=FEMA_Clean11_df[FEMA_Clean11_df[\"elevationdifference\"] != \"N/A\"]\n",
    "FEMA_Clean12_df.dropna(subset=['elevationdifference'], how='all', inplace=True)\n",
    "print(FEMA_Clean12_df.shape)\n",
    "\n",
    "FEMA_Clean13_df=FEMA_Clean12_df[FEMA_Clean12_df[\"state\"] != \"N/A\"]\n",
    "FEMA_Clean13_df.dropna(subset=['state'], how='all', inplace=True)\n",
    "print(FEMA_Clean13_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.3. Check to make sure there is no empty cells in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with any empty cells, check\n",
    "FEMA_Clean14_df=FEMA_Clean13_df.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)\n",
    "print(FEMA_Clean14_df.shape)\n",
    "FEMA_Clean14_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Check FEMA DataFrame Data-types and convert dates to DateTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEMA_Clean14_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert dateofloss type to datetime\n",
    "FEMA_Clean14_df['dateofloss'] =  pd.to_datetime(FEMA_Clean14_df['dateofloss'], format='%Y-%m-%d')\n",
    "FEMA_Clean14_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify which row makes error on originalconstructiondate and doesnt let to change the type to date\n",
    "FEMA_Clean14_df['originalconstructiondate'] =  pd.to_datetime(FEMA_Clean14_df['originalconstructiondate'], errors='coerce')\n",
    "FEMA_Clean14_df.sort_values('originalconstructiondate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEMA_Clean14_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Due to processing time, limit the Data to only 3 years (2013-2015) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEMA_Clean15_df=FEMA_Clean14_df[FEMA_Clean14_df[\"dateofloss\"] > \"2012-12-31\"]\n",
    "FEMA_Clean15_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEMA_Clean15_df=FEMA_Clean15_df[FEMA_Clean15_df[\"dateofloss\"] < \"2016-01-01\"]\n",
    "FEMA_Clean15_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEMA_Clean15_df.sort_values('dateofloss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D.1. Remove the row in originalconstructiondate that causes error, this does not happen for years 2014 to 2015\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the row in originalconstructiondate that causes error, this does not happen for years 2014 to 2015\n",
    "# FEMA_Clean15_df=FEMA_Clean14_df.drop(549916)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert originalconstructiondate type to datetime\n",
    "FEMA_Clean15_df['originalconstructiondate'] =  pd.to_datetime(FEMA_Clean15_df['originalconstructiondate'], format='%Y-%m-%d', errors='ignore')\n",
    "FEMA_Clean15_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E. Convert FEMA DataFram to GeoDataFrame\n",
    "This is done to create a \"point\" for each house that was damaged. Later we can use this point to find the closest weather station to damaged house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopandas import GeoDataFrame\n",
    "from shapely.geometry import Point\n",
    "import pandas as pd\n",
    "import geopandas\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_FEMA = geopandas.GeoDataFrame(\n",
    "    FEMA_Clean15_df, geometry=geopandas.points_from_xy(FEMA_Clean15_df.longitude, FEMA_Clean15_df.latitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_FEMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NASA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import *\n",
    "import netCDF4\n",
    "import xarray as xr\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Read Data and Create a 1 combined DataFrame (Limit the data to only 3 years due to long processing time)\n",
    "\n",
    "The NASA 3 years will match the FEMA 3 years (2013-2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.1. Read NASA datat in .nc4 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_2010 = xr.open_dataset('NASA/data/daymet_v3_stnsxval_prcp_2010.nc4',drop_variables=[\"stns\",\"station_id\",\"station_name\",\"stnz\",\"time_bnds\", \"stnx\",\"stny\"])\n",
    "# ds_2011 = xr.open_dataset('NASA/data/daymet_v3_stnsxval_prcp_2011.nc4',drop_variables=[\"stns\",\"station_id\",\"station_name\",\"stnz\",\"time_bnds\", \"stnx\",\"stny\"])\n",
    "# ds_2012 = xr.open_dataset('NASA/data/daymet_v3_stnsxval_prcp_2012.nc4',drop_variables=[\"stns\",\"station_id\",\"station_name\",\"stnz\",\"time_bnds\", \"stnx\",\"stny\"])\n",
    "ds_2013 = xr.open_dataset('NASA/data/daymet_v3_stnsxval_prcp_2013.nc4',drop_variables=[\"stns\",\"station_id\",\"station_name\",\"stnz\",\"time_bnds\", \"stnx\",\"stny\"])\n",
    "ds_2014 = xr.open_dataset('NASA/data/daymet_v3_stnsxval_prcp_2014.nc4',drop_variables=[\"stns\",\"station_id\",\"station_name\",\"stnz\",\"time_bnds\", \"stnx\",\"stny\"])\n",
    "ds_2015 = xr.open_dataset('NASA/data/daymet_v3_stnsxval_prcp_2015.nc4',drop_variables=[\"stns\",\"station_id\",\"station_name\",\"stnz\",\"time_bnds\", \"stnx\",\"stny\"])\n",
    "ds_2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.2. Save data in a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_2010 = ds_2010.to_dataframe()\n",
    "# ds_2011 = ds_2011.to_dataframe()\n",
    "# ds_2012 = ds_2012.to_dataframe()\n",
    "ds_2013 = ds_2013.to_dataframe()\n",
    "ds_2014 = ds_2014.to_dataframe()\n",
    "ds_2015 = ds_2015.to_dataframe()\n",
    "ds_2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.3. Reset index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_2010=ds_2010.reset_index()\n",
    "# ds_2011=ds_2011.reset_index()\n",
    "# ds_2012=ds_2012.reset_index()\n",
    "ds_2013=ds_2013.reset_index()\n",
    "ds_2014=ds_2014.reset_index()\n",
    "ds_2015=ds_2015.reset_index()\n",
    "ds_2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.4. Combine the NASA DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NASA_Comb = pd.concat([\n",
    "ds_2013,\n",
    "ds_2014,\n",
    "ds_2015], ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Convert timestep column to Date Column\n",
    "\n",
    "This is necessary, because the FEMA dataset has only the \"Data\" column, which will be used to merge the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NASA_Comb['time'] = pd.to_datetime(NASA_Comb['time'])\n",
    "NASA_Comb['new_date_column'] = NASA_Comb['time'].dt.date\n",
    "NASA_Comb['new_date_column'] =  pd.to_datetime(NASA_Comb['new_date_column'], format='%Y-%m-%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NASA_Comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NASA_Comb.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Convert DataFrame to GeoDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_NASA = geopandas.GeoDataFrame(\n",
    "    NASA_Comb, geometry=geopandas.points_from_xy(NASA_Comb.stn_lon, NASA_Comb.stn_lat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdf_NASA.to_csv('gdf_NASA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdf_NASA=pd.read_csv('gdf_NASA.csv')\n",
    "# gdf_NASA.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Convert all nan Data on \"obs\" Column to \"0\"\n",
    "\n",
    "obs represents the amout the rainfall that the weather station has recorded on a specific date\n",
    "Many obs data was not recorded and the cell was empty. Therefore it was assumed that there was no rain on that specific date. We do underestand that this may cause some inaccuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_NASA['obs'] = gdf_NASA['obs'].fillna(0.0)\n",
    "gdf_NASA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine FEMA and NASA Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Create a Function to find the closest distance between the points between FEMA and NASA Data, and Merge the 2 datasets\n",
    "\n",
    "The function creates a new column called \"dist\", which represents the disctance between the 2 \"Points\" from each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.spatial import cKDTree\n",
    "from shapely.geometry import Point\n",
    "\n",
    "gpd1 = gpd.GeoDataFrame([['John', 1, Point(41.7, -70.0)]],\n",
    "                        \n",
    "                        columns=['Name', 'ID', 'geometry'])\n",
    "gpd2 = gpd.GeoDataFrame([['Work', Point(41.6, -69.99)]],\n",
    "                        \n",
    "                        columns=['Place', 'geometry'])\n",
    "\n",
    "def ckdnearest(gdA, gdB):\n",
    "    nA = np.array(list(zip(gdA.geometry.x, gdA.geometry.y)) )\n",
    "    nB = np.array(list(zip(gdB.geometry.x, gdB.geometry.y)) )\n",
    "    btree = cKDTree(nB)\n",
    "    dist, idx = btree.query(nA, k=1)\n",
    "    gdf = pd.concat(\n",
    "        [gdA.reset_index(drop=True), gdB.loc[idx, gdB.columns != 'geometry'].reset_index(drop=True),\n",
    "         pd.Series(dist, name='dist')], axis=1)\n",
    "    return gdf\n",
    "\n",
    "ckdnearest(gpd1, gpd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "math.sqrt((0.01)**2 + 0.1 **2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the function\n",
    "gdf_FEMA_plus_station_id=ckdnearest(gdf_FEMA, gdf_NASA) #gdf_NASA[:10000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Merge FEMA and NASA datasets\n",
    "\n",
    "The fist merged dataset is created based on the closest distance between points. \n",
    "From this merge we can find the closest wether station to each damaged hourse. See column \"stns\"\n",
    "But the function doesn not look at matching the date from FEMA \"dateofloss\" and the date from NASA \"new_date_column\"\n",
    "\n",
    "Considering we were able to match the weather station \"stns\" from NASA to FEMA database, we need to do the following:\n",
    "\n",
    "### B.1. Remove Columns from \"first merged\" dataset that does not have correct value \"time\", \"obs\", \"pred\", \"stn_lat\", \"stn_lon\", \"new_date_column\", \"dist\", \"geometry\". \n",
    "\n",
    "### B.2. Keep Station column \"stns\" in the \"first merged\" dataset \n",
    "\n",
    "### B.3 Calculate the comulative amount of rain in the last 2 days and save it in a new column\n",
    "\n",
    "### B.4. Merge NASA dataset again with \"first merged\" dataset based on the Station and Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_FEMA_plus_station_id.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.1. Remove Columns from \"first merged\" dataset that does not have correct value.\n",
    "### B.2. Keep Station column \"stns\" in the \"first merged\" dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_FEMA_plus_station_id=gdf_FEMA_plus_station_id.drop(columns=[\"time\",\"obs\",\"pred\",\"stn_lat\",\"stn_lon\",\"new_date_column\",\"dist\",\"geometry\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf_FEMA_plus_station_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.3 Calculate the comulative amount of rain in the last 2 days and save it in a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_NASA['rolling_2days_obs'] = gdf_NASA['obs'].rolling(2).sum()\n",
    "gdf_NASA['rolling_7days_obs'] = gdf_NASA['obs'].rolling(7).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.4. Merge NASA dataset again with \"first merged\" dataset based on the Station and Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(gdf_FEMA_plus_station_id, gdf_NASA, left_on=['stns', 'dateofloss'], right_on=['stns', 'new_date_column'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Combined NASA and FEMA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_clean_df=merged_df.drop(columns=[\"time\"])\n",
    "merged_clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_clean_df.to_csv('merged_clean_df.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
