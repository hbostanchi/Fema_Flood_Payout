{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import operator\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our input dataset\n",
    "#data1 = Path('/Users/halleh/Desktop/Resources/Fima_Flood_Payout/data_processing/merged_clean_3Years_df.csv')\n",
    "data2 = Path('/Users/halleh/Desktop/Resources/UCB_Final_Project/Data/FEMA_cleaned14.csv')\n",
    "FEMA_Clean14_df = pd.read_csv(data2)\n",
    "FEMA_Clean14_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert date columns to datetime64[ns] \n",
    "#only keep the builtyear and lossyear insted of the date\n",
    "FEMA_Clean14_df['builtyear'] = pd.DatetimeIndex(FEMA_Clean14_df['originalconstructiondate']).year\n",
    "FEMA_Clean14_df['lossyear'] = pd.DatetimeIndex(FEMA_Clean14_df['dateofloss']).year\n",
    "FEMA_Clean14_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEMA_df=FEMA_Clean14_df.drop(['dateofloss',\t'numberoffloorsintheinsuredbuilding' ,'elevationdifference', 'year', 'reportedcity','originalconstructiondate', 'state','month', 'season' , 'latitude','longitude' ], axis = 1)#\n",
    "FEMA_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head(3)\n",
    "FEMA_encoded = pd.get_dummies(FEMA_df, columns=[\"floodzone\", \"occupancytype\"]) #\"state\"\n",
    "#FEMA_encoded = pd.get_dummies(df, columns=[\"amountpaidonbuildingclaim\"])\n",
    "FEMA_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating a correlation matrix\n",
    "corrMatrix = FEMA_df.corr()\n",
    "\n",
    "import seaborn as sn\n",
    "sn.heatmap(corrMatrix, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y =  FEMA_encoded['amountpaidonbuildingclaim'] #.values\n",
    "X =  FEMA_encoded.copy()#.values\n",
    "X =  FEMA_encoded.drop(['amountpaidonbuildingclaim'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model with scikit-learn\n",
    "model = LinearRegression()\n",
    "# By convention, X is capitalized and y is lowercase\n",
    "model.fit(X, y)\n",
    "\n",
    "# The model creates predicted y values based on X values\n",
    "y_pred = model.predict(X)\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the preprocessed data into a training and testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression() \n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "score_paid= model.score(X_test, y_test)\n",
    "print (score_paid)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "df_paidpred = pd.DataFrame({'Actual Payoffs': y_test, 'Predicted Payoffs': y_pred})\n",
    "df_paidpred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    random_state=1, \n",
    "                                                    stratify=y)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression(solver='lbfgs', random_state=1)\n",
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "results = pd.DataFrame({\"Prediction\": y_pred, \"Actual\": y_test}).reset_index(drop=True)\n",
    "results.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_features= PolynomialFeatures(degree=2)\n",
    "x_poly = polynomial_features.fit_transform(X)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(x_poly, y)\n",
    "y_poly_pred = model.predict(x_poly)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y,y_poly_pred))\n",
    "r2 = r2_score(y,y_poly_pred)\n",
    "print(rmse)\n",
    "print(r2)\n",
    "\n",
    "plt.scatter(x, y, s=10)\n",
    "# sort the values of x before line plot\n",
    "sort_axis = operator.itemgetter(0)\n",
    "sorted_zip = sorted(zip(x,y_poly_pred), key=sort_axis)\n",
    "X, y_poly_pred = zip(*sorted_zip)\n",
    "plt.plot(X, y_poly_pred, color='m')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def create_polynomial_regression_model(degree):\n",
    "  \"Creates a polynomial regression model for the given degree\"\n",
    "  \n",
    "  poly_features = PolynomialFeatures(degree=degree)\n",
    "  \n",
    "  # transforms the existing features to higher degree features.\n",
    "  X_train_poly = poly_features.fit_transform(X_train)\n",
    "  \n",
    "  # fit the transformed features to Linear Regression\n",
    "  poly_model = LinearRegression()\n",
    "  poly_model.fit(X_train_poly, y_train)\n",
    "  \n",
    "  # predicting on training data-set\n",
    "  y_train_predicted = poly_model.predict(X_train_poly)\n",
    "  \n",
    "  # predicting on test data-set\n",
    "  y_test_predict = poly_model.predict(poly_features.fit_transform(X_test))\n",
    "  \n",
    "  # evaluating the model on training dataset\n",
    "  rmse_train = np.sqrt(mean_squared_error(y_train, y_train_predicted))\n",
    "  r2_train = r2_score(y_train, y_train_predicted)\n",
    "  \n",
    "  # evaluating the model on test dataset\n",
    "  rmse_test = np.sqrt(mean_squared_error(y_test, y_test_predict))\n",
    "  r2_test = r2_score(y_test, y_test_predict)\n",
    "  \n",
    "  print(\"The model performance for the training set\")\n",
    "  print(\"-------------------------------------------\")\n",
    "  print(\"RMSE of training set is {}\".format(rmse_train))\n",
    "  print(\"R2 score of training set is {}\".format(r2_train))\n",
    "  \n",
    "  print(\"\\n\")\n",
    "  \n",
    "  print(\"The model performance for the test set\")\n",
    "  print(\"-------------------------------------------\")\n",
    "  print(\"RMSE of test set is {}\".format(rmse_test))\n",
    "  print(\"R2 score of test set is {}\".format(r2_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_polynomial_regression_model(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the train_test_split function to create training and testing subsets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    random_state=1, \n",
    "                                                    stratify=y)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the data\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using the test data\n",
    "y_pred = model.predict(X_test)\n",
    "results = pd.DataFrame({\n",
    "    \"Prediction\": y_pred, \n",
    "    \"Actual\": y_test\n",
    "}).reset_index(drop=True)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Cryptocurrencies Using K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import plotly.express as px\n",
    "import hvplot.pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.Create an elbow curve to find the best value for K, and use the pcs_df DataFrame.\n",
    "# Find the best value for K\n",
    "inertia = []\n",
    "k = list(range(1, 11))\n",
    "\n",
    "# Calculate the inertia for the range of K values\n",
    "for i in k:\n",
    "    km = KMeans(n_clusters=i, random_state=0)\n",
    "    km.fit(FEMA_encoded)\n",
    "    inertia.append(km.inertia_)\n",
    "\n",
    "# Create the elbow curve\n",
    "elbow_data = {\"k\": k, \"inertia\": inertia}\n",
    "df_elbow = pd.DataFrame(elbow_data)\n",
    "df_elbow.hvplot.line(x=\"k\", y=\"inertia\", xticks=k, title=\"Elbow Curve\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_clusters = get_clusters(5, shopping_df)\n",
    "five_clusters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "six_clusters = get_clusters(5, shopping_df)\n",
    "six_clusters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_clusters.hvplot.scatter(x=\"Annual Income\", y=\"Spending Score (1-100)\" , by=\"class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the 3D-scatter with x=\"Annual Income\", y=\"Spending Score (1-100)\" and z=\"Age\"\n",
    "fig = px.scatter_3d(\n",
    "    five_clusters,\n",
    "    x=\"Age\",\n",
    "    y=\"Spending Score (1-100)\",\n",
    "    z=\"Annual Income\",\n",
    "    color=\"class\",\n",
    "    symbol=\"class\",\n",
    "    width=800,\n",
    ")\n",
    "fig.update_layout(legend=dict(x=0, y=1))\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gradient_boosted_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features set\n",
    "X = FEMA_encoded.copy()\n",
    "X = X.drop(\"amountpaidonbuildingclaim\", axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target vector\n",
    "y = FEMA_encoded[\"amountpaidonbuildingclaim\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Splitting into Train and Test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    random_state=1)\n",
    "\n",
    "# Creating StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fitting Standard Scaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scaling data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Create a classifier object\n",
    "learning_rates = [0.05, 0.1, 0.25, 0.5, 0.75, 1]\n",
    "for learning_rate in learning_rates:\n",
    "    classifier = GradientBoostingClassifier(n_estimators=20,\n",
    "                                            learning_rate=learning_rate,\n",
    "                                            max_features=5,\n",
    "                                            max_depth=3,\n",
    "                                            random_state=0)\n",
    "\n",
    "    # Fit the model\n",
    "    classifier.fit(X_train_scaled, y_train)\n",
    "    print(\"Learning rate: \", learning_rate)\n",
    "\n",
    "    # Score the model\n",
    "    print(\"Accuracy score (training): {0:.3f}\".format(\n",
    "        classifier.score(\n",
    "            X_train_scaled,\n",
    "            y_train)))\n",
    "    print(\"Accuracy score (validation): {0:.3f}\".format(\n",
    "        classifier.score(\n",
    "            X_test_scaled,\n",
    "            y_test)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a learning rate and create classifier\n",
    "classifier = GradientBoostingClassifier(n_estimators=20,\n",
    "                                        learning_rate=0.5,\n",
    "                                        max_features=5,\n",
    "                                        max_depth=3,\n",
    "                                        random_state=0)\n",
    "\n",
    "# Fit the model\n",
    "classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make Prediction\n",
    "predictions = classifier.predict(X_test_scaled)\n",
    "pd.DataFrame({\"Prediction\": predictions, \"Actual\": y_test}).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "# Calculating the accuracy score\n",
    "acc_score = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy Score : {acc_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "cm_df = pd.DataFrame(\n",
    "    cm, index=[\"Actual 0\", \"Actual 1\"],\n",
    "    columns=[\"Predicted 0\", \"Predicted 1\"]\n",
    ")\n",
    "\n",
    "# Displaying results\n",
    "display(cm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification report\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
